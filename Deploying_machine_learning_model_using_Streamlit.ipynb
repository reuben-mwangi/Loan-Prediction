{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deploying machine learning model using Streamlit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMBm7q0cHLtOLdgs66BEoN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reuben-mwangi/Loan-Prediction/blob/main/Deploying_machine_learning_model_using_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of Machine Learning Lifecycle\n",
        "Let’s start with understanding the overall machine learning lifecycle, and the different steps that are\n",
        "involved in creating a machine learning project. Broadly, the entire machine learning lifecycle can be\n",
        "described as a combination of 6 stages\n",
        "\n",
        "##1.Stage 1: Problem Definition \n",
        " The first and most important part of any project is to define the problem statement. Here, we want to\n",
        "describe the aim or the goal of our project and what we want to achieve at the end.\n",
        "##2.Stage 2: Hypothesis Generation\n",
        " Once the problem statement is finalized, we move on to the hypothesis generation part. Here, we try to\n",
        "point out the factors/features that can help us to solve the problem at hand.\n",
        "##3. Stage 3: Data Collection\n",
        "\n",
        "After generating hypotheses, we get the list of features that are useful for a problem. Next, we collect the\n",
        "data accordingly. This data can be collected from different sources.\n",
        "\n",
        "##Stage 4: Data Exploration and Pre-processing\n",
        "After collecting the data, we move on to explore and pre-process it. These steps help us to generate\n",
        "meaningful insights from the data. We also clean the dataset in this step, before building the model.\n",
        "## Stage 5: Model Building\n",
        "Once we have explored and pre-processed the dataset, the next step is to build the model. Here, we create\n",
        "predictive models in order to build a solution for the project.\n",
        "\n",
        "##Stage 6: Model Deployment\n",
        "Once you have the solution, you want to showcase it and make it accessible for others. And hence, the\n",
        "final stage of the machine learning lifecycle is to deploy that model.\n",
        "These are the 6 stages of a machine learning lifecycle. The aim of this project is to understand the last\n",
        "stage, i.e. model deployment, in detail using streamlit. However, I will briefly explain the remaining stages\n",
        "and the complete machine learning lifecycle along with their implementation in Python, before diving deep\n",
        "into the model deployment part using streamlit.\n",
        "\n",
        "## Understanding the Problem Statement: Automating Loan Prediction\n",
        "The project that I have picked is automating the loan eligibility process. The task is\n",
        "to predict whether the loan will be approved or not based on the details provided by customers. Here is the\n",
        "problem statement for this project:\n",
        "### Automate the loan eligibility process based on customer details provided while filling online application form.\n",
        "\n",
        "Based on the details provided by customers, we have to create a model that can decide where or not their\n",
        "loan should be approved. This completes the problem definition part of the first stage of the machine\n",
        "learning lifecycle. The next step is to generate hypotheses and point out the factors that will help us to\n",
        "predict whether the loan for a customer should be approved or not.\n",
        "As a starting point, here are a couple of factors that I think will be helpful for us with respect to this\n",
        "project:\n",
        "Amount of loan: The total amount of loan applied by the customer. My hypothesis here is that the\n",
        "higher the amount of loan, the lesser will be the chances of loan approval and vice versa.\n",
        "Income of applicant: The income of the applicant (customer) can also be a deciding factor. A higher\n",
        "income will lead to higher probability of loan approval.\n",
        "Education of applicant: Educational qualification of the applicant can also be a vital factor to predict\n",
        "the loan status of a customer. My hypothesis is if the educational qualification of the applicant is\n",
        "higher, the chances of their loan approval will be higher.\n",
        "These are some factors that can be useful to predict the loan status of a customer. Obviously, this is a very\n",
        "small list, and you can come up with many more hypotheses. But, since the focus of this article is on\n",
        "model deployment, I will leave this hypothesis generation part for you to explore further.\n",
        "Next, we need to collect the data. We know certain features that we want like the income details,\n",
        "educational qualification, and so on. \n",
        "We have some variables related to the loan, like the loan ID, which is the unique ID for each customer, Loan\n",
        "Amount and Loan Amount Term, which tells us the amount of loan in thousands and the term of the loan in\n",
        "months respectively. Credit History represents whether a customer has any previous unclear debts or not.\n",
        "Apart from this, we have customer details as well, like their Gender, Marital Status, Educational\n",
        "qualification, income, and so on. Using these features, we will create a predictive model that will predict\n",
        "the target variable which is Loan Status representing whether the loan will be approved or not.\n",
        "Now we have finalized the problem statement, generated the hypotheses, and collected the data. Next are\n",
        "the Data exploration and pre-processing phase. Here, we will explore the dataset and pre-process it. The\n",
        "common steps under this step are as follows:\n",
        "1.Univariate Analysis\n",
        "2.Bivariate Analysis\n",
        "3.Missing Value Treatment\n",
        "4.Outlier Treatment\n",
        "5.Feature Engineering\n",
        "We explore the variables individually which is called the univariate analysis. Exploring the effect of one\n",
        "variable on the other, or exploring two variables at a time is the bivariate analysis. We also look for any\n",
        "missing values or outliers that might be present in the dataset and deal with them. And we might also\n",
        "create new features using the existing features which are referred to as feature engineering. Again, I will\n",
        "not focus much on these data exploration parts and will only do the necessary pre-processing.\n",
        "After exploring and pre-processing the data, next comes the model building phase. Since it is a\n",
        "classification problem, we can use any of the classification models like the logistic regression, decision\n",
        "tree, random forest, etc. I have tried all of these 3 models for this problem and random forest produced the\n",
        "best results. So, I will use a random forest as the predictive model for this project.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GInSv2eVgcRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning model for Automating Loan Prediction"
      ],
      "metadata": {
        "id": "AR5-unm7jCCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/train_ctrUa4K.csv\")\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "LmJ_me5gi-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the first five rows from the dataset. We know that machine learning models take only numbers as\n",
        "inputs and can not process strings. So, we have to deal with the categories present in the dataset and\n",
        "convert them into numbers."
      ],
      "metadata": {
        "id": "J1KS6IIrj_MY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['Gender']= train['Gender'].map({'Male':0, 'Female':1})\n",
        "train['Married']= train['Married'].map({'No':0, 'Yes':1})\n",
        "train['Loan_Status']= train['Loan_Status'].map({'N':0, 'Y':1})"
      ],
      "metadata": {
        "id": "32-DiCwekBuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have converted the categories present in the Gender, Married and the Loan Status variable into\n",
        "numbers, simply using the map function of python. Next, let’s check if there are any missing values in the\n",
        "dataset:"
      ],
      "metadata": {
        "id": "PamfckDkkWlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "SlqjxUAdkdRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, there are missing values on many variables including the Gender, Married, LoanAmount variable. Next,\n",
        "we will remove all the rows which contain any missing values in them:"
      ],
      "metadata": {
        "id": "3d_y79c2knae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.dropna()\n",
        "\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "rN4yqex5kuiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there are no missing values in the dataset. Next, we will separate the dependent (Loan_Status) and\n",
        "the independent variables:\n"
      ],
      "metadata": {
        "id": "9LYt_ZU5k3ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x= train[[\"Gender\",\"Married\",\"ApplicantIncome\",\"LoanAmount\",\"Credit_History\"]]\n",
        "\n",
        "y= train.Loan_Status\n",
        "\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "ocgOkbiok7hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this particular project, I have only picked 5 variables that I think are most relevant. These are the\n",
        "Gender, Marital Status, ApplicantIncome, LoanAmount, and Credit_History and stored them in variable X.\n",
        "Target variable is stored in another variable y. And there are 480 observations available. Next, let’s move on\n",
        "to the model building stage.\n",
        "Here, we will first split our dataset into a training and validation set, so that we can train the model on the\n",
        "training set and evaluate its performance on the validation set.\n"
      ],
      "metadata": {
        "id": "Ik1lvnNslcNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_cv,y_train,y_cv = train_test_split(x,y,test_size = 0.2,random_state = 10)"
      ],
      "metadata": {
        "id": "0kafL8m8llhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have split the data using the train_test_split function from the sklearn library keeping the test_size as\n",
        "0.2 which means 20 percent of the total dataset will be kept aside for the validation set. Next, we will train\n",
        "the random forest model using the training set:"
      ],
      "metadata": {
        "id": "piZWENhnmGFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        " model = RandomForestClassifier(max_depth= 4,random_state= 10)\n",
        "\n",
        " model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "SAfXLQg6mQJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I have kept the max_depth as 4 for each of the trees of our random forest and stored the trained\n",
        "model in a variable named model. Now, our model is trained, let’s check its performance on both the\n",
        "training and validation set:"
      ],
      "metadata": {
        "id": "FLpjLXWWmoG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred_cv = model.predict(x_cv)\n",
        "\n",
        "accuracy_score(y_cv,pred_cv)"
      ],
      "metadata": {
        "id": "tcMjNhcvmuEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is 80% accurate on the validation set. Let’s check the performance on the training set too:"
      ],
      "metadata": {
        "id": "Gn6vw4ZSnEr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_train = model.predict(x_train)\n",
        "accuracy_score(y_train, pred_train)"
      ],
      "metadata": {
        "id": "xS80FFXpnIxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance on the training set is almost similar to that on the validation set. So, the model has\n",
        "generalized well. Finally, we will save this trained model so that it can be used in the future to make\n",
        "predictions on new observations."
      ],
      "metadata": {
        "id": "IIT79RxInXlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"classifier.pkl\",mode=\"wb\")\n",
        "\n",
        "pickle.dump(model,pickle_out)\n",
        "\n",
        "pickle_out.close()"
      ],
      "metadata": {
        "id": "SeDywk9BnebK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are saving the model in pickle format and storing it as classifier.pkl. This will store the trained model\n",
        "and we will use this while deploying the model.\n",
        "This completes the first five stages of the machine learning lifecycle. Next, we will explore the last stage\n",
        "which is model deployment. We will be deploying this loan prediction model so that it can be accessed by\n",
        "others. And to do so, we will use Streamlit which is a recent and the simplest way of building web apps and\n",
        "deploying machine learning and deep learning models.\n",
        "\n"
      ],
      "metadata": {
        "id": "kAZUG8n-n6P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Streamlit\n",
        "\n",
        "As per the founders of Streamlit, it is the fastest way to build data apps and share them. It is a recent\n",
        "model deployment tool that simplifies the entire model deployment cycle and lets you deploy your models\n",
        "quickly. I have been exploring this tool for the past couple of weeks and as per my experience, it is a\n",
        "simple, quick, and interpretable model deployment tool.\n",
        "Here are some of the key features of Streamlit which I found really interesting and useful:\n",
        "1. It quickly turns data scripts into shareable web applications. You just have to pass a running script to\n",
        "the tool and it can convert that to a web app.\n",
        "2. Everything in Python. The best thing about Streamlit is that everything we do is in Python. Starting\n",
        "from loading the model to creating the frontend, all can be done using Python.\n",
        "3. All for free. It is open source and hence no cost is involved. You can deploy your apps without paying\n",
        "for them.\n",
        "4. No front-end experience required. Model deployment generally contains two parts, frontend, and\n",
        "backend. The backend is generally a working model, a machine learning model in our case, which is\n",
        "built-in python. And the front end part, which generally requires some knowledge of other languages\n",
        "like java scripts, etc. Using Streamlit, we can create this front end in Python itself. So, we need not\n",
        "pred_train = model.predict(x_train)\n",
        "accuracy_score(y_train,pred_train)\n",
        "# saving the model\n",
        "import pickle\n",
        "pickle_out = open(\"classifier.pkl\", mode = \"wb\")\n",
        "pickle.dump(model, pickle_out)\n",
        "pickle_out.close()\n",
        "view raw\n",
        "learn any other programming languages or web development techniques. Understanding Python is\n",
        "enough.\n",
        "Let’s say we are deploying the model without using Streamlit. In that case, the entire pipeline will look\n",
        "something like this:\n",
        "Model Building\n",
        "Creating a python script\n",
        "Write Flask app\n",
        "Create front-end: JavaScript\n",
        "Deploy\n",
        "We will first build our model and convert it into a python script. Then we will have to create the web app\n",
        "using let’s say flask. We will also have to create the front end for the web app and here we will have to use\n",
        "JavaScript. And then finally, we will deploy the model. So, if you would notice, we will require the\n",
        "knowledge of Python to build the model and then a thorough understanding of JavaScript and flask to\n",
        "build the front end and deploying the model. Now, let’s look at the deployment pipeline if we use Streamlit:\n",
        "Model Building\n",
        "Creating a python script\n",
        "Create front-end: Python\n",
        "Deploy\n",
        "Here we will build the model and create a python script for it. Then we will build the front-end for the app\n",
        "which will be in python and finally, we will deploy the model. \n",
        "\n"
      ],
      "metadata": {
        "id": "aZ4sMOEioJS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment of the Loan Prediction model using Streamlit"
      ],
      "metadata": {
        "id": "SA8Dorq3o3Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we start by installing basics\n",
        "!pip install -q pyngrok\n",
        "\n",
        "!pip install -q streamlit\n",
        "\n",
        "!pip install -q streamlit_ace\n"
      ],
      "metadata": {
        "id": "g4cRD-wyoHY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have installed 3 libraries here. pyngrok is a python wrapper for ngrok which helps to open secure\n",
        "tunnels from public URLs to localhost. This will help us to host our web app. Streamlit will be used to make\n",
        "our web app.\n",
        "Next, we will have to create a separate session in Streamlit for our app. You can download the\n",
        "sessionstate.py file from here and store that in your current working directory. This will help you to create\n",
        "a session for your app. Finally, we have to create the python script for our app. Let me show the code first\n",
        "and then I will explain it to you in detail:"
      ],
      "metadata": {
        "id": "gUCn8UHMpnuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import pickle\n",
        "import streamlit as st\n",
        "\n",
        "# loading the trained model\n",
        "\n",
        "pickle_in = open(\"classifier.pkl\",\"rb\")\n",
        "\n",
        "classifier = pickle.load(pickle_in)\n",
        "\n",
        "@st.cache()\n",
        "\n",
        "# defining the functions will make the prediction using the data which the user inputs\n",
        "\n",
        "def prediction(Gender,Married,ApplicantIncome,Credit_History):\n",
        "\n",
        "  #pre-processing user input\n",
        "  if Gender == \"Male\":\n",
        "          Gender=0\n",
        "  else:\n",
        "          Gender = 1\n",
        "if Married == \"Unmarried\":\n",
        "     Married = 0\n",
        "else:\n",
        "     married = 1\n",
        "if Credit_History == \"Unclear Debts\":\n",
        "    Credit_History = 0\n",
        "else:\n",
        "     Credit_History = 1\n",
        "LoanAmount = LoanAmount/1000\n",
        "\n",
        "\n",
        "# making Predictions\n",
        "\n",
        "prediction = classifier.predict(\n",
        "    [[Gender,Married,ApplicantIncome,LoanAmount,Credit_History]]\n",
        ")\n",
        "\n",
        "if prediction == 0:\n",
        "      pred = \"Rejected\"\n",
        "else:\n",
        "      pred = \"Approved\"\n",
        "return pred\n",
        "\n",
        "\n",
        "# this is the main function in which we define our webpage\n",
        "\n",
        "\n",
        "def main():\n",
        "  # front end elements of the web page\n",
        "  html_temp = \"\"\"\n",
        "  <div style = \"background-color:yellow;padding:13px\">\n",
        "  <h1 style = \"color:black;text-align:center;>Streamlit Loan Prediction ML App</h1>\n",
        "  <div>\n",
        "  \"\"\"\n",
        "\n",
        "  #display the front end aspect\n",
        "  st.markdown(html_temp,unsafe_allow_html = True)\n",
        "\n",
        "  # following lines create boxes in which user can enter data required to make prediction\n",
        "  Gender = st.selectbox(\"Gender\",(\"Male\",\"Female\"))\n",
        "  Married = st.selectbox(\"Marital_Status\",(\"Unmarried\",\"Married\"))\n",
        "  ApplicantIncome = st.number_input(\"Applicants monthly income\")\n",
        "  loanAmount = st.number_input(\"Total loan amount\")\n",
        "  Credit_History = st.selectbox(\"Credit_History\",(\"Unclear Debts\",\"No unclear Debts\"))\n",
        "  result = \"\"\n",
        "\n",
        "  # when \"predict\" is clicked,make the prediction and store it\n",
        "\n",
        "  if st.button(\"predict\"):\n",
        "    result = prediction(Gender,Married,ApplicantIncome,LoanAmount,Credit_History)\n",
        "    st.success(\"Your loan is {}\".format(result))\n",
        "    print(LoanAmount)\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "  main()\n",
        "  "
      ],
      "metadata": {
        "id": "3KGNtZI2puBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alright, let’s now host this app to a public URL using pyngrok library."
      ],
      "metadata": {
        "id": "Oqg3D2P9v8_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py &>dev/null&"
      ],
      "metadata": {
        "id": "RljdnxIvwA33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are first running the python script. And then we will connect it to a public URL:"
      ],
      "metadata": {
        "id": "nDcPmAVcwPLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(\"8501\")\n",
        "\n",
        "public_url"
      ],
      "metadata": {
        "id": "yUnxnF3FwVbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##This will generate a link something like this:"
      ],
      "metadata": {
        "id": "vmcWA9vcwmBA"
      }
    }
  ]
}